# Testing Specification

> **Role**: QA / Test Engineer
> **Created**: 2025-12-04
> **Version**: 1.0

---

## 1. Testing Strategy

### 1.1 í…ŒìŠ¤íŠ¸ í”¼ë¼ë¯¸ë“œ

```mermaid
flowchart TB
    subgraph E2E["ğŸ”º E2E Tests (10%)"]
        E1[ì „ì²´ ê²€ìƒ‰ í”Œë¡œìš°]
        E2[ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸]
    end

    subgraph Integration["ğŸ”· Integration Tests (30%)"]
        I1[Storm API ì—°ë™]
        I2[ChromaDB ì €ì¥ì†Œ]
        I3[ê²€ìƒ‰ ì„œë¹„ìŠ¤]
    end

    subgraph Unit["ğŸ”» Unit Tests (60%)"]
        U1[ì²­í‚¹]
        U2[ì„ë² ë”©]
        U3[RRF ìœµí•©]
        U4[í•˜ì´ë¼ì´íŠ¸]
        U5[ëª¨ë¸ ê²€ì¦]
    end

    E2E --> Integration
    Integration --> Unit
```

### 1.2 í…ŒìŠ¤íŠ¸ ë²”ìœ„

| ê³„ì¸µ | ëŒ€ìƒ | ì»¤ë²„ë¦¬ì§€ ëª©í‘œ |
|------|------|--------------|
| Unit | ìˆœìˆ˜ í•¨ìˆ˜, ëª¨ë¸, ìœ í‹¸ | 90% |
| Integration | ì™¸ë¶€ ì„œë¹„ìŠ¤ ì—°ë™ | 80% |
| E2E | ì£¼ìš” ì‚¬ìš©ì í”Œë¡œìš° | 70% |
| ì „ì²´ | - | 85% |

---

## 2. Test Structure

### 2.1 ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py                 # ê³µí†µ í”½ìŠ¤ì²˜
â”œâ”€â”€ pytest.ini                  # pytest ì„¤ì •
â”‚
â”œâ”€â”€ unit/                       # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_models.py          # Pydantic ëª¨ë¸
â”‚   â”œâ”€â”€ test_chunker.py         # í…ìŠ¤íŠ¸ ì²­í‚¹
â”‚   â”œâ”€â”€ test_embedder.py        # ì„ë² ë”© ìƒì„±
â”‚   â”œâ”€â”€ test_bm25.py            # BM25 ì¸ë±ìŠ¤
â”‚   â”œâ”€â”€ test_rrf.py             # RRF ìœµí•©
â”‚   â”œâ”€â”€ test_highlighter.py     # í•˜ì´ë¼ì´íŠ¸
â”‚   â”œâ”€â”€ test_preprocessor.py    # ì¿¼ë¦¬ ì „ì²˜ë¦¬
â”‚   â””â”€â”€ test_utils.py           # ìœ í‹¸ë¦¬í‹°
â”‚
â”œâ”€â”€ integration/                # í†µí•© í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_storm_client.py    # Storm Parse API
â”‚   â”œâ”€â”€ test_chroma_store.py    # ChromaDB
â”‚   â”œâ”€â”€ test_search_service.py  # ê²€ìƒ‰ ì„œë¹„ìŠ¤
â”‚   â””â”€â”€ test_pipeline.py        # ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸
â”‚
â”œâ”€â”€ e2e/                        # E2E í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_search_flow.py     # ê²€ìƒ‰ ì „ì²´ í”Œë¡œìš°
â”‚   â””â”€â”€ test_ingest_flow.py     # ìˆ˜ì§‘ ì „ì²´ í”Œë¡œìš°
â”‚
â””â”€â”€ fixtures/                   # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    â”œâ”€â”€ sample.pdf              # ìƒ˜í”Œ PDF
    â”œâ”€â”€ parsed_pages.json       # íŒŒì‹± ê²°ê³¼
    â”œâ”€â”€ chunks.json             # ì²­í¬ ë°ì´í„°
    â””â”€â”€ embeddings.npy          # ì„ë² ë”© ë²¡í„°
```

### 2.2 pytest ì„¤ì •

```ini
# pytest.ini

[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# ë§ˆì»¤
markers =
    unit: Unit tests
    integration: Integration tests
    e2e: End-to-end tests
    slow: Slow tests
    external: Tests requiring external services

# ë¹„ë™ê¸°
asyncio_mode = auto

# ì¶œë ¥
addopts =
    -v
    --tb=short
    --strict-markers
    -ra
    --cov=src/bookbrain
    --cov-report=term-missing
    --cov-report=html:htmlcov
    --cov-fail-under=85

# í™˜ê²½
env =
    STORM_API_KEY=test_key
    OPENAI_API_KEY=test_key
    DATA_DIR=/tmp/bookbrain_test
    LOG_LEVEL=DEBUG
```

---

## 3. Fixtures (`conftest.py`)

### 3.1 ê³µí†µ í”½ìŠ¤ì²˜

```python
# tests/conftest.py

import os
import pytest
import asyncio
from pathlib import Path
from typing import Generator, AsyncGenerator

import chromadb
from chromadb.config import Settings as ChromaSettings

from bookbrain.core.config import Settings
from bookbrain.models.document import Book, Chapter, ParsedPage
from bookbrain.models.chunk import Chunk, ChunkMetadata
from bookbrain.models.search import SearchQuery, SearchMode


# ============ Session Fixtures ============

@pytest.fixture(scope="session")
def event_loop():
    """ì„¸ì…˜ ë²”ìœ„ ì´ë²¤íŠ¸ ë£¨í”„"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()


@pytest.fixture(scope="session")
def test_data_dir(tmp_path_factory) -> Path:
    """ì„¸ì…˜ ë²”ìœ„ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë””ë ‰í† ë¦¬"""
    return tmp_path_factory.mktemp("bookbrain_test")


# ============ Settings Fixture ============

@pytest.fixture
def settings(test_data_dir: Path) -> Settings:
    """í…ŒìŠ¤íŠ¸ìš© ì„¤ì •"""
    return Settings(
        storm_api_key="test_storm_key",
        openai_api_key="test_openai_key",
        data_dir=test_data_dir,
        chunk_size=200,  # í…ŒìŠ¤íŠ¸ìš© ì‘ì€ ì‚¬ì´ì¦ˆ
        chunk_overlap=50,
        log_level="DEBUG",
    )


# ============ Model Fixtures ============

@pytest.fixture
def sample_book() -> Book:
    """ìƒ˜í”Œ Book ê°ì²´"""
    return Book(
        id="test_book",
        title="í…ŒìŠ¤íŠ¸ ì±…",
        file_name="test_book.pdf",
        file_hash="a" * 64,
        total_pages=10,
        chapters=[
            Chapter(title="1ì¥ ì†Œê°œ", level=1, start_page=1),
            Chapter(title="2ì¥ ë³¸ë¡ ", level=1, start_page=5),
        ],
        language="ko",
    )


@pytest.fixture
def sample_pages() -> list[ParsedPage]:
    """ìƒ˜í”Œ íŒŒì‹±ëœ í˜ì´ì§€"""
    return [
        ParsedPage(
            page_number=1,
            content="## 1ì¥ ì†Œê°œ\n\nì´ê²ƒì€ ì²« ë²ˆì§¸ í˜ì´ì§€ì…ë‹ˆë‹¤.\nìë°” ìŠ¤íŠ¸ë¦¼ APIì— ëŒ€í•´ ì•Œì•„ë´…ë‹ˆë‹¤.",
            chapter_title="1ì¥ ì†Œê°œ",
        ),
        ParsedPage(
            page_number=2,
            content="### 1.1 ìŠ¤íŠ¸ë¦¼ì´ë€\n\nìŠ¤íŠ¸ë¦¼ì€ ë°ì´í„° ì²˜ë¦¬ ì—°ì‚°ì„ ì§€ì›í•©ë‹ˆë‹¤.\n```java\nList<String> names = list.stream().collect(toList());\n```",
            chapter_title="1ì¥ ì†Œê°œ",
        ),
        ParsedPage(
            page_number=3,
            content="## 2ì¥ ë³¸ë¡ \n\nëŒë‹¤ í‘œí˜„ì‹ì„ í™œìš©í•œ í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë°.",
            chapter_title="2ì¥ ë³¸ë¡ ",
        ),
    ]


@pytest.fixture
def sample_chunks() -> list[Chunk]:
    """ìƒ˜í”Œ ì²­í¬ ëª©ë¡"""
    return [
        Chunk(
            id="chunk_001",
            text="ìë°” 8ì—ì„œ ì œê³µí•˜ëŠ” ìŠ¤íŠ¸ë¦¼ APIëŠ” ë°ì´í„° ì²˜ë¦¬ ì—°ì‚°ì„ ì§€ì›í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.",
            metadata=ChunkMetadata(
                book_id="modern_java",
                book_title="ëª¨ë˜ ìë°” ì¸ ì•¡ì…˜",
                book_file="modern_java.pdf",
                chapter="1ì¥ ì†Œê°œ",
                page_start=1,
                page_end=1,
                chunk_index=0,
                content_type="text",
            ),
            embedding=[0.1] * 1536,
        ),
        Chunk(
            id="chunk_002",
            text="ëŒë‹¤ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ë©´ ê°„ê²°í•˜ê³  ìœ ì—°í•œ ì½”ë“œë¥¼ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            metadata=ChunkMetadata(
                book_id="modern_java",
                book_title="ëª¨ë˜ ìë°” ì¸ ì•¡ì…˜",
                book_file="modern_java.pdf",
                chapter="2ì¥ ëŒë‹¤",
                page_start=10,
                page_end=11,
                chunk_index=1,
                content_type="text",
            ),
            embedding=[0.2] * 1536,
        ),
        Chunk(
            id="chunk_003",
            text="Optional í´ë˜ìŠ¤ëŠ” nullì„ ëŒ€ì‹ í•˜ì—¬ ê°’ì˜ ì¡´ì¬ ì—¬ë¶€ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í‘œí˜„í•©ë‹ˆë‹¤.",
            metadata=ChunkMetadata(
                book_id="modern_java",
                book_title="ëª¨ë˜ ìë°” ì¸ ì•¡ì…˜",
                book_file="modern_java.pdf",
                chapter="3ì¥ Optional",
                page_start=20,
                page_end=20,
                chunk_index=2,
                content_type="text",
            ),
            embedding=[0.3] * 1536,
        ),
    ]


@pytest.fixture
def sample_query() -> SearchQuery:
    """ìƒ˜í”Œ ê²€ìƒ‰ ì¿¼ë¦¬"""
    return SearchQuery(
        text="ìŠ¤íŠ¸ë¦¼ API ì‚¬ìš©ë²•",
        mode=SearchMode.HYBRID,
        top_k=10,
        book_filter=None,
        vector_weight=0.5,
    )


# ============ Storage Fixtures ============

@pytest.fixture
def chroma_client(test_data_dir: Path) -> chromadb.Client:
    """í…ŒìŠ¤íŠ¸ìš© ChromaDB í´ë¼ì´ì–¸íŠ¸ (Ephemeral)"""
    return chromadb.EphemeralClient(
        settings=ChromaSettings(
            anonymized_telemetry=False,
            allow_reset=True,
        )
    )


@pytest.fixture
def chroma_collection(chroma_client: chromadb.Client):
    """í…ŒìŠ¤íŠ¸ìš© ChromaDB ì»¬ë ‰ì…˜"""
    collection = chroma_client.get_or_create_collection(
        name="test_collection",
        metadata={"hnsw:space": "cosine"},
    )
    yield collection
    # Cleanup
    chroma_client.delete_collection("test_collection")


# ============ Mock Fixtures ============

@pytest.fixture
def mock_storm_response() -> dict:
    """Mock Storm Parse API ì‘ë‹µ"""
    return {
        "jobId": "test_job_123",
        "state": "COMPLETED",
        "requestedAt": "2025-12-04T00:00:00Z",
        "completedAt": "2025-12-04T00:01:00Z",
        "pages": [
            {"pageNumber": 1, "content": "## Chapter 1\n\nContent of page 1"},
            {"pageNumber": 2, "content": "### Section 1.1\n\nContent of page 2"},
        ],
    }


@pytest.fixture
def mock_embeddings() -> list[list[float]]:
    """Mock ì„ë² ë”© ë²¡í„°"""
    import numpy as np
    np.random.seed(42)
    return [np.random.randn(1536).tolist() for _ in range(10)]
```

---

## 4. Unit Tests

### 4.1 Models (`test_models.py`)

```python
# tests/unit/test_models.py

import pytest
from pydantic import ValidationError

from bookbrain.models.document import Book, Chapter, ParsedPage
from bookbrain.models.chunk import Chunk, ChunkMetadata
from bookbrain.models.search import SearchQuery, SearchResult, SearchMode


class TestBookModel:
    """Book ëª¨ë¸ í…ŒìŠ¤íŠ¸"""

    def test_valid_book(self, sample_book):
        """ìœ íš¨í•œ Book ìƒì„±"""
        assert sample_book.id == "test_book"
        assert sample_book.total_pages == 10
        assert len(sample_book.chapters) == 2

    def test_book_id_pattern(self):
        """book_id íŒ¨í„´ ê²€ì¦"""
        with pytest.raises(ValidationError) as exc_info:
            Book(
                id="Invalid ID!",  # íŠ¹ìˆ˜ë¬¸ì í¬í•¨
                title="Test",
                file_name="test.pdf",
                file_hash="a" * 64,
                total_pages=1,
            )

        assert "pattern" in str(exc_info.value)

    def test_book_file_hash_length(self):
        """file_hash ê¸¸ì´ ê²€ì¦"""
        with pytest.raises(ValidationError):
            Book(
                id="test",
                title="Test",
                file_name="test.pdf",
                file_hash="short",  # 64ì ë¯¸ë§Œ
                total_pages=1,
            )


class TestChunkMetadata:
    """ChunkMetadata ëª¨ë¸ í…ŒìŠ¤íŠ¸"""

    def test_page_range_validation(self):
        """page_end >= page_start ê²€ì¦"""
        with pytest.raises(ValidationError):
            ChunkMetadata(
                book_id="test",
                book_title="Test",
                book_file="test.pdf",
                chapter="1ì¥",
                page_start=10,
                page_end=5,  # startë³´ë‹¤ ì‘ìŒ
                chunk_index=0,
                content_type="text",
            )

    def test_to_chroma_metadata(self):
        """ChromaDB ë©”íƒ€ë°ì´í„° ë³€í™˜"""
        meta = ChunkMetadata(
            book_id="test",
            book_title="Test",
            book_file="test.pdf",
            chapter=None,
            page_start=1,
            page_end=1,
            chunk_index=0,
            content_type="text",
        )

        chroma_meta = meta.to_chroma_metadata()

        assert chroma_meta["book_id"] == "test"
        assert chroma_meta["chapter"] == ""  # None â†’ ë¹ˆ ë¬¸ìì—´


class TestChunk:
    """Chunk ëª¨ë¸ í…ŒìŠ¤íŠ¸"""

    def test_generate_id(self):
        """ì²­í¬ ID ìƒì„±"""
        id1 = Chunk.generate_id("book1", 0)
        id2 = Chunk.generate_id("book1", 1)
        id3 = Chunk.generate_id("book1", 0)  # ë™ì¼ ì…ë ¥

        assert id1 != id2  # ë‹¤ë¥¸ indexëŠ” ë‹¤ë¥¸ ID
        assert id1 == id3  # ë™ì¼ ì…ë ¥ì€ ë™ì¼ ID
        assert len(id1) == 16  # 16ìë¦¬ í•´ì‹œ

    def test_embedding_dimension(self):
        """ì„ë² ë”© ì°¨ì› ê²€ì¦"""
        with pytest.raises(ValidationError):
            Chunk(
                id="test",
                text="Test content",
                metadata=ChunkMetadata(
                    book_id="test",
                    book_title="Test",
                    book_file="test.pdf",
                    page_start=1,
                    page_end=1,
                    chunk_index=0,
                ),
                embedding=[0.1] * 100,  # 1536ì´ ì•„ë‹Œ 100
            )


class TestSearchQuery:
    """SearchQuery ëª¨ë¸ í…ŒìŠ¤íŠ¸"""

    def test_default_values(self):
        """ê¸°ë³¸ê°’ í™•ì¸"""
        query = SearchQuery(text="test")

        assert query.mode == SearchMode.HYBRID
        assert query.top_k == 10
        assert query.vector_weight == 0.5
        assert query.book_filter is None

    def test_top_k_range(self):
        """top_k ë²”ìœ„ ê²€ì¦"""
        with pytest.raises(ValidationError):
            SearchQuery(text="test", top_k=0)

        with pytest.raises(ValidationError):
            SearchQuery(text="test", top_k=101)

    def test_vector_weight_range(self):
        """vector_weight ë²”ìœ„ ê²€ì¦"""
        with pytest.raises(ValidationError):
            SearchQuery(text="test", vector_weight=1.5)
```

### 4.2 Chunker (`test_chunker.py`)

```python
# tests/unit/test_chunker.py

import pytest

from bookbrain.ingestion.chunker import TextChunker
from bookbrain.models.document import ParsedPage


class TestTextChunker:
    """TextChunker í…ŒìŠ¤íŠ¸"""

    def test_basic_chunking(self, settings, sample_pages):
        """ê¸°ë³¸ ì²­í‚¹"""
        chunker = TextChunker(settings)

        chunks = list(chunker.chunk_pages(
            pages=sample_pages,
            book_id="test",
            book_title="Test Book",
            book_file="test.pdf",
        ))

        assert len(chunks) > 0
        assert all(c.id for c in chunks)
        assert all(c.text for c in chunks)
        assert all(c.metadata.book_id == "test" for c in chunks)

    def test_chunk_overlap(self, settings):
        """ì²­í¬ ì˜¤ë²„ë© í™•ì¸"""
        # ê¸´ í…ìŠ¤íŠ¸ ìƒì„±
        long_text = "í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤. " * 100
        pages = [ParsedPage(page_number=1, content=long_text)]

        chunker = TextChunker(settings)
        chunks = list(chunker.chunk_pages(pages, "test", "Test", "test.pdf"))

        # ì˜¤ë²„ë© ì¡´ì¬ í™•ì¸ (ì¸ì ‘ ì²­í¬ ê°„ ê³µí†µ í…ìŠ¤íŠ¸)
        if len(chunks) >= 2:
            overlap_found = False
            for i in range(len(chunks) - 1):
                if chunks[i].text[-50:] in chunks[i + 1].text[:100]:
                    overlap_found = True
                    break
            # ì˜¤ë²„ë© ì„¤ì •ì´ ìˆìœ¼ë©´ ë°œê²¬ë˜ì–´ì•¼ í•¨
            assert overlap_found or settings.chunk_overlap == 0

    def test_code_block_preservation(self, settings):
        """ì½”ë“œ ë¸”ë¡ ë³´ì¡´"""
        code_content = """## ì½”ë“œ ì˜ˆì œ

```python
def hello():
    print("Hello, World!")
    return True
```

ìœ„ ì½”ë“œëŠ” ê°„ë‹¨í•œ ì˜ˆì œì…ë‹ˆë‹¤.
"""
        pages = [ParsedPage(page_number=1, content=code_content)]

        chunker = TextChunker(settings)
        chunks = list(chunker.chunk_pages(pages, "test", "Test", "test.pdf"))

        # ì½”ë“œ ë¸”ë¡ì´ ì™„ì „íˆ í¬í•¨ëœ ì²­í¬ê°€ ìˆì–´ì•¼ í•¨
        full_code = "def hello():"
        assert any(full_code in c.text for c in chunks)

    def test_page_range_tracking(self, settings, sample_pages):
        """í˜ì´ì§€ ë²”ìœ„ ì¶”ì """
        chunker = TextChunker(settings)
        chunks = list(chunker.chunk_pages(
            pages=sample_pages,
            book_id="test",
            book_title="Test",
            book_file="test.pdf",
        ))

        for chunk in chunks:
            assert chunk.metadata.page_start >= 1
            assert chunk.metadata.page_end >= chunk.metadata.page_start
            assert chunk.metadata.page_end <= len(sample_pages)

    def test_content_type_detection(self, settings):
        """ì½˜í…ì¸  íƒ€ì… ê°ì§€"""
        code_page = ParsedPage(
            page_number=1,
            content="```java\npublic class Test {}\n```\n```java\nSystem.out.println();\n```",
        )
        text_page = ParsedPage(
            page_number=2,
            content="ì´ê²ƒì€ ì¼ë°˜ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.",
        )

        chunker = TextChunker(settings)

        code_chunks = list(chunker.chunk_pages([code_page], "t", "T", "t.pdf"))
        text_chunks = list(chunker.chunk_pages([text_page], "t", "T", "t.pdf"))

        # ì½”ë“œê°€ ë§ì€ ì²­í¬ëŠ” 'code' ë˜ëŠ” 'mixed'
        assert any(c.metadata.content_type in ("code", "mixed") for c in code_chunks)
        # í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ì²­í¬ëŠ” 'text'
        assert all(c.metadata.content_type == "text" for c in text_chunks)

    def test_empty_pages(self, settings):
        """ë¹ˆ í˜ì´ì§€ ì²˜ë¦¬"""
        pages = [ParsedPage(page_number=1, content="   \n\n   ")]

        chunker = TextChunker(settings)
        chunks = list(chunker.chunk_pages(pages, "t", "T", "t.pdf"))

        # ë¹ˆ í˜ì´ì§€ëŠ” ì²­í¬ê°€ ìƒì„±ë˜ì§€ ì•Šì•„ì•¼ í•¨
        assert len(chunks) == 0

    def test_chunk_index_sequential(self, settings, sample_pages):
        """ì²­í¬ ì¸ë±ìŠ¤ ìˆœì°¨ì  ì¦ê°€"""
        chunker = TextChunker(settings)
        chunks = list(chunker.chunk_pages(
            pages=sample_pages,
            book_id="test",
            book_title="Test",
            book_file="test.pdf",
        ))

        indices = [c.metadata.chunk_index for c in chunks]
        assert indices == list(range(len(chunks)))
```

### 4.3 RRF Fusion (`test_rrf.py`)

```python
# tests/unit/test_rrf.py

import pytest
from dataclasses import dataclass

from bookbrain.search.rrf_fusion import RRFFusion


@dataclass
class MockVectorResult:
    chunk_id: str
    text: str
    similarity: float
    rank: int
    metadata: dict


@dataclass
class MockKeywordResult:
    chunk_id: str
    text: str
    score: float
    rank: int
    metadata: dict


class TestRRFFusion:
    """RRF Fusion í…ŒìŠ¤íŠ¸"""

    @pytest.fixture
    def fusion(self):
        return RRFFusion(k=60)

    @pytest.fixture
    def vector_results(self):
        return [
            MockVectorResult("A", "Doc A", 0.95, 1, {"book_id": "b1"}),
            MockVectorResult("B", "Doc B", 0.85, 2, {"book_id": "b1"}),
            MockVectorResult("C", "Doc C", 0.75, 3, {"book_id": "b1"}),
        ]

    @pytest.fixture
    def keyword_results(self):
        return [
            MockKeywordResult("B", "Doc B", 12.5, 1, {"book_id": "b1"}),
            MockKeywordResult("D", "Doc D", 10.0, 2, {"book_id": "b1"}),
            MockKeywordResult("A", "Doc A", 8.0, 3, {"book_id": "b1"}),
        ]

    def test_basic_fusion(self, fusion, vector_results, keyword_results):
        """ê¸°ë³¸ RRF ìœµí•©"""
        results = fusion.fuse(vector_results, keyword_results)

        # ê²°ê³¼ ì¡´ì¬
        assert len(results) > 0

        # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ
        scores = [r.rrf_score for r in results]
        assert scores == sorted(scores, reverse=True)

    def test_both_sources_combined(self, fusion, vector_results, keyword_results):
        """ì–‘ìª½ ì†ŒìŠ¤ì—ì„œ ë‚˜ì˜¨ ê²°ê³¼ í†µí•©"""
        results = fusion.fuse(vector_results, keyword_results)

        # A, BëŠ” ì–‘ìª½ì—ì„œ ë‚˜ì˜´ â†’ ì ìˆ˜ í•©ì‚°
        result_a = next(r for r in results if r.chunk_id == "A")
        result_b = next(r for r in results if r.chunk_id == "B")

        # ì–‘ìª½ ì ìˆ˜ ëª¨ë‘ ìˆì–´ì•¼ í•¨
        assert result_a.vector_rank is not None
        assert result_a.keyword_rank is not None
        assert result_b.vector_rank is not None
        assert result_b.keyword_rank is not None

    def test_single_source_result(self, fusion, vector_results, keyword_results):
        """í•œìª½ì—ì„œë§Œ ë‚˜ì˜¨ ê²°ê³¼"""
        results = fusion.fuse(vector_results, keyword_results)

        # CëŠ” ë²¡í„°ì—ì„œë§Œ
        result_c = next(r for r in results if r.chunk_id == "C")
        assert result_c.vector_rank is not None
        assert result_c.keyword_rank is None

        # DëŠ” í‚¤ì›Œë“œì—ì„œë§Œ
        result_d = next(r for r in results if r.chunk_id == "D")
        assert result_d.vector_rank is None
        assert result_d.keyword_rank is not None

    def test_rrf_formula(self, fusion):
        """RRF ê³µì‹ ê²€ì¦"""
        # score = 1 / (k + rank)

        # ë²¡í„° rank 1, í‚¤ì›Œë“œ rank 3
        expected_a = (1 / (60 + 1)) + (1 / (60 + 3))
        # = 0.01639 + 0.01587 = 0.03226

        vector = [MockVectorResult("A", "A", 0.9, 1, {})]
        keyword = [MockKeywordResult("A", "A", 10, 3, {})]

        results = fusion.fuse(vector, keyword, alpha=0.5)
        result_a = results[0]

        assert abs(result_a.rrf_score - expected_a * 0.5 * 2) < 0.0001

    def test_alpha_weighting(self, fusion, vector_results, keyword_results):
        """alpha ê°€ì¤‘ì¹˜ í…ŒìŠ¤íŠ¸"""
        # alpha=1.0: ë²¡í„°ë§Œ
        results_vector = fusion.fuse(vector_results, keyword_results, alpha=1.0)

        # alpha=0.0: í‚¤ì›Œë“œë§Œ
        results_keyword = fusion.fuse(vector_results, keyword_results, alpha=0.0)

        # ë²¡í„° ìš°ì„  ì‹œ ë²¡í„° 1ë“±ì´ ìµœìƒìœ„
        assert results_vector[0].chunk_id == "A"

        # í‚¤ì›Œë“œ ìš°ì„  ì‹œ í‚¤ì›Œë“œ 1ë“±ì´ ìµœìƒìœ„
        assert results_keyword[0].chunk_id == "B"

    def test_empty_results(self, fusion):
        """ë¹ˆ ê²°ê³¼ ì²˜ë¦¬"""
        results = fusion.fuse([], [])
        assert results == []

        results = fusion.fuse([], [MockKeywordResult("A", "A", 10, 1, {})])
        assert len(results) == 1

    def test_k_parameter_effect(self):
        """k íŒŒë¼ë¯¸í„° íš¨ê³¼"""
        vector = [
            MockVectorResult("A", "A", 0.9, 1, {}),
            MockVectorResult("B", "B", 0.8, 2, {}),
        ]

        # kê°€ ì‘ì„ìˆ˜ë¡ ìˆœìœ„ ì°¨ì´ ì˜í–¥ í¼
        fusion_small_k = RRFFusion(k=1)
        fusion_large_k = RRFFusion(k=100)

        results_small = fusion_small_k.fuse(vector, [])
        results_large = fusion_large_k.fuse(vector, [])

        # ì ìˆ˜ ì°¨ì´ ê³„ì‚°
        diff_small = results_small[0].rrf_score - results_small[1].rrf_score
        diff_large = results_large[0].rrf_score - results_large[1].rrf_score

        # kê°€ ì‘ì„ìˆ˜ë¡ ì ìˆ˜ ì°¨ì´ í¼
        assert diff_small > diff_large
```

### 4.4 Highlighter (`test_highlighter.py`)

```python
# tests/unit/test_highlighter.py

import pytest

from bookbrain.search.highlighter import Highlighter


class TestHighlighter:
    """Highlighter í…ŒìŠ¤íŠ¸"""

    @pytest.fixture
    def highlighter(self):
        return Highlighter(
            pre_tag="<mark>",
            post_tag="</mark>",
            max_length=200,
        )

    def test_basic_highlight(self, highlighter):
        """ê¸°ë³¸ í•˜ì´ë¼ì´íŠ¸"""
        text = "ìë°” ìŠ¤íŠ¸ë¦¼ APIëŠ” ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë„êµ¬ì…ë‹ˆë‹¤."
        query = "ìŠ¤íŠ¸ë¦¼"

        result = highlighter.highlight(text, query, mode="full")

        assert "<mark>ìŠ¤íŠ¸ë¦¼</mark>" in result

    def test_multiple_matches(self, highlighter):
        """ì—¬ëŸ¬ ë§¤ì¹­ í•˜ì´ë¼ì´íŠ¸"""
        text = "ìŠ¤íŠ¸ë¦¼ì€ ìŠ¤íŠ¸ë¦¼ APIë¥¼ í†µí•´ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ë¥¼ í•©ë‹ˆë‹¤."
        query = "ìŠ¤íŠ¸ë¦¼"

        result = highlighter.highlight(text, query, mode="full")

        assert result.count("<mark>ìŠ¤íŠ¸ë¦¼</mark>") == 3

    def test_case_insensitive(self, highlighter):
        """ëŒ€ì†Œë¬¸ì ë¬´ì‹œ"""
        text = "Stream APIì™€ STREAM ì²˜ë¦¬, stream ì—°ì‚°"
        query = "stream"

        result = highlighter.highlight(text, query, mode="full")

        assert "<mark>Stream</mark>" in result
        assert "<mark>STREAM</mark>" in result
        assert "<mark>stream</mark>" in result

    def test_snippet_mode(self, highlighter):
        """ìŠ¤ë‹ˆí« ëª¨ë“œ"""
        text = "A" * 100 + " ìŠ¤íŠ¸ë¦¼ API " + "B" * 100
        query = "ìŠ¤íŠ¸ë¦¼"

        result = highlighter.highlight(text, query, mode="snippet")

        # ìµœëŒ€ ê¸¸ì´ ì´ë‚´
        assert len(result) <= highlighter.max_length + 50  # ì—¬ìœ ë¶„

        # ë§¤ì¹­ í¬í•¨
        assert "<mark>ìŠ¤íŠ¸ë¦¼</mark>" in result

    def test_multiple_query_terms(self, highlighter):
        """ì—¬ëŸ¬ ê²€ìƒ‰ì–´"""
        text = "ìë°” 8ì˜ ìŠ¤íŠ¸ë¦¼ APIëŠ” ëŒë‹¤ì™€ í•¨ê»˜ ì‚¬ìš©í•©ë‹ˆë‹¤."
        query = "ìŠ¤íŠ¸ë¦¼ ëŒë‹¤"

        result = highlighter.highlight(text, query, mode="full")

        assert "<mark>ìŠ¤íŠ¸ë¦¼</mark>" in result
        assert "<mark>ëŒë‹¤</mark>" in result

    def test_no_match(self, highlighter):
        """ë§¤ì¹­ ì—†ìŒ"""
        text = "ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤."
        query = "ì—†ëŠ”ë‹¨ì–´"

        result = highlighter.highlight(text, query, mode="full")

        assert "<mark>" not in result
        assert result == text

    def test_empty_query(self, highlighter):
        """ë¹ˆ ê²€ìƒ‰ì–´"""
        text = "í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤."
        query = ""

        result = highlighter.highlight(text, query, mode="full")

        assert result == text

    def test_special_characters(self, highlighter):
        """íŠ¹ìˆ˜ë¬¸ì ì´ìŠ¤ì¼€ì´í”„"""
        text = "ì •ê·œì‹ì—ì„œ . * + ë“±ì€ íŠ¹ìˆ˜ë¬¸ìì…ë‹ˆë‹¤."
        query = "."

        # ì—ëŸ¬ ì—†ì´ ì‹¤í–‰
        result = highlighter.highlight(text, query, mode="full")
        assert result  # ê²°ê³¼ ì¡´ì¬

    def test_korean_tokens(self, highlighter):
        """í•œê¸€ í† í°"""
        text = "ìë°”ìŠ¤í¬ë¦½íŠ¸ëŠ” ìë°”ì™€ ë‹¤ë¥¸ ì–¸ì–´ì…ë‹ˆë‹¤."
        query = "ìë°”"

        result = highlighter.highlight(text, query, mode="full")

        # "ìë°”ìŠ¤í¬ë¦½íŠ¸"ì˜ "ìë°”"ë„ í•˜ì´ë¼ì´íŠ¸
        assert "<mark>ìë°”</mark>" in result
```

---

## 5. Integration Tests

### 5.1 ChromaDB Store (`test_chroma_store.py`)

```python
# tests/integration/test_chroma_store.py

import pytest

from bookbrain.storage.chroma_store import ChromaStore


@pytest.mark.integration
class TestChromaStore:
    """ChromaStore í†µí•© í…ŒìŠ¤íŠ¸"""

    @pytest.fixture
    def store(self, settings):
        """í…ŒìŠ¤íŠ¸ìš© ChromaStore"""
        store = ChromaStore(settings)
        store.initialize()
        yield store
        store.clear()  # Cleanup

    def test_add_and_get_chunk(self, store, sample_chunks):
        """ì²­í¬ ì¶”ê°€ ë° ì¡°íšŒ"""
        # ì¶”ê°€
        count = store.add_chunks(sample_chunks)
        assert count == len(sample_chunks)

        # ì¡°íšŒ
        chunk = store.get_chunk(sample_chunks[0].id)
        assert chunk is not None
        assert chunk.text == sample_chunks[0].text

    def test_search(self, store, sample_chunks):
        """ë²¡í„° ê²€ìƒ‰"""
        store.add_chunks(sample_chunks)

        # ì²« ë²ˆì§¸ ì²­í¬ì˜ ì„ë² ë”©ìœ¼ë¡œ ê²€ìƒ‰
        query_embedding = sample_chunks[0].embedding

        results = store.search(
            query_embedding=query_embedding,
            top_k=3,
        )

        # ê²°ê³¼ ì¡´ì¬
        assert len(results) > 0

        # ì²« ë²ˆì§¸ ê²°ê³¼ê°€ ê°€ì¥ ìœ ì‚¬
        assert results[0][0] == sample_chunks[0].id

    def test_search_with_filter(self, store, sample_chunks):
        """í•„í„°ë§ ê²€ìƒ‰"""
        store.add_chunks(sample_chunks)

        # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” book_idë¡œ í•„í„°
        results = store.search(
            query_embedding=sample_chunks[0].embedding,
            top_k=10,
            book_filter=["nonexistent_book"],
        )

        # ê²°ê³¼ ì—†ìŒ
        assert len(results) == 0

    def test_delete_book(self, store, sample_chunks):
        """ì±… ì‚­ì œ"""
        store.add_chunks(sample_chunks)

        # ì‚­ì œ
        deleted = store.delete_book("modern_java")

        assert deleted == len(sample_chunks)

        # í™•ì¸
        chunk = store.get_chunk(sample_chunks[0].id)
        assert chunk is None

    def test_get_stats(self, store, sample_chunks):
        """í†µê³„ ì¡°íšŒ"""
        store.add_chunks(sample_chunks)

        stats = store.get_stats()

        assert stats["total_chunks"] == len(sample_chunks)
        assert stats["total_books"] == 1
        assert len(stats["books"]) == 1

    def test_upsert_existing(self, store, sample_chunks):
        """ê¸°ì¡´ ì²­í¬ ì—…ë°ì´íŠ¸"""
        # ì²« ì¶”ê°€
        store.add_chunks(sample_chunks)

        # ë™ì¼ IDë¡œ ì¬ì¶”ê°€ (ë‚´ìš© ë³€ê²½)
        modified_chunk = sample_chunks[0].model_copy(update={
            "text": "ìˆ˜ì •ëœ í…ìŠ¤íŠ¸"
        })
        store.add_chunks([modified_chunk])

        # í™•ì¸
        chunk = store.get_chunk(modified_chunk.id)
        assert chunk.text == "ìˆ˜ì •ëœ í…ìŠ¤íŠ¸"

        # ì´ ê°œìˆ˜ ë™ì¼
        stats = store.get_stats()
        assert stats["total_chunks"] == len(sample_chunks)
```

### 5.2 Search Service (`test_search_service.py`)

```python
# tests/integration/test_search_service.py

import pytest
from unittest.mock import AsyncMock, patch

from bookbrain.search.service import SearchService
from bookbrain.models.search import SearchQuery, SearchMode


@pytest.mark.integration
class TestSearchService:
    """SearchService í†µí•© í…ŒìŠ¤íŠ¸"""

    @pytest.fixture
    async def service(self, settings, sample_chunks):
        """í…ŒìŠ¤íŠ¸ìš© SearchService"""
        from bookbrain.storage.chroma_store import ChromaStore
        from bookbrain.storage.bm25_index import BM25Index

        chroma = ChromaStore(settings)
        chroma.initialize()
        chroma.add_chunks(sample_chunks)

        bm25 = BM25Index(settings)
        bm25.build(sample_chunks)

        service = SearchService(settings, chroma, bm25)

        yield service

        # Cleanup
        chroma.clear()

    @pytest.mark.asyncio
    async def test_hybrid_search(self, service):
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        with patch.object(
            service._embedder,
            'embed',
            new_callable=AsyncMock,
            return_value=[0.1] * 1536,
        ):
            query = SearchQuery(
                text="ìŠ¤íŠ¸ë¦¼ API",
                mode=SearchMode.HYBRID,
                top_k=5,
            )

            response = await service.search(query)

            assert len(response.results) > 0
            assert response.search_time_ms > 0
            assert response.query == query

    @pytest.mark.asyncio
    async def test_vector_only_search(self, service):
        """ë²¡í„° ê²€ìƒ‰ë§Œ"""
        with patch.object(
            service._embedder,
            'embed',
            new_callable=AsyncMock,
            return_value=[0.1] * 1536,
        ):
            query = SearchQuery(
                text="ëŒë‹¤ í‘œí˜„ì‹",
                mode=SearchMode.VECTOR,
                top_k=5,
            )

            response = await service.search(query)

            # ë²¡í„° ì ìˆ˜ë§Œ ì¡´ì¬
            for result in response.results:
                assert result.score_vector is not None
                assert result.score_bm25 is None

    @pytest.mark.asyncio
    async def test_keyword_only_search(self, service):
        """í‚¤ì›Œë“œ ê²€ìƒ‰ë§Œ"""
        query = SearchQuery(
            text="Optional",
            mode=SearchMode.KEYWORD,
            top_k=5,
        )

        response = await service.search(query)

        # í‚¤ì›Œë“œ ì ìˆ˜ë§Œ ì¡´ì¬
        for result in response.results:
            if result.score_bm25 is not None:
                assert result.score_vector is None

    @pytest.mark.asyncio
    async def test_highlight_in_results(self, service):
        """ê²°ê³¼ì— í•˜ì´ë¼ì´íŠ¸ í¬í•¨"""
        query = SearchQuery(
            text="ìŠ¤íŠ¸ë¦¼",
            mode=SearchMode.KEYWORD,
            top_k=5,
        )

        response = await service.search(query)

        # í•˜ì´ë¼ì´íŠ¸ëœ í…ìŠ¤íŠ¸ ì¡´ì¬
        for result in response.results:
            if "ìŠ¤íŠ¸ë¦¼" in result.text:
                assert result.highlighted_text is not None
                assert "<mark>" in result.highlighted_text
```

---

## 6. E2E Tests

### 6.1 Search Flow (`test_search_flow.py`)

```python
# tests/e2e/test_search_flow.py

import pytest
from pathlib import Path

from bookbrain.core.config import get_settings
from bookbrain.ingestion.pipeline import IngestionPipeline
from bookbrain.search.service import SearchService
from bookbrain.models.search import SearchQuery, SearchMode


@pytest.mark.e2e
@pytest.mark.slow
class TestSearchFlow:
    """ê²€ìƒ‰ ì „ì²´ í”Œë¡œìš° E2E í…ŒìŠ¤íŠ¸"""

    @pytest.fixture(scope="class")
    def ingested_service(self, tmp_path_factory):
        """ìˆ˜ì§‘ ì™„ë£Œëœ ê²€ìƒ‰ ì„œë¹„ìŠ¤"""
        # ì„ì‹œ ë””ë ‰í† ë¦¬
        test_dir = tmp_path_factory.mktemp("e2e_test")

        # ì„¤ì •
        settings = get_settings()
        settings.data_dir = test_dir

        # ìƒ˜í”Œ PDF ìˆ˜ì§‘ (fixtures ì‚¬ìš©)
        sample_pdf = Path(__file__).parent.parent / "fixtures" / "sample.pdf"

        if sample_pdf.exists():
            pipeline = IngestionPipeline(settings)
            import asyncio
            asyncio.run(pipeline.ingest(sample_pdf))

        # ì„œë¹„ìŠ¤ ë°˜í™˜
        from bookbrain.storage.chroma_store import ChromaStore
        from bookbrain.storage.bm25_index import BM25Index

        chroma = ChromaStore(settings)
        chroma.initialize()

        bm25 = BM25Index(settings)
        bm25.load()

        return SearchService(settings, chroma, bm25)

    @pytest.mark.asyncio
    async def test_full_search_flow(self, ingested_service):
        """ì „ì²´ ê²€ìƒ‰ í”Œë¡œìš°"""
        query = SearchQuery(
            text="í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ì–´",
            mode=SearchMode.HYBRID,
            top_k=5,
        )

        response = await ingested_service.search(query)

        # ì‘ë‹µ êµ¬ì¡° ê²€ì¦
        assert response is not None
        assert hasattr(response, 'results')
        assert hasattr(response, 'search_time_ms')

    @pytest.mark.asyncio
    async def test_search_result_structure(self, ingested_service):
        """ê²€ìƒ‰ ê²°ê³¼ êµ¬ì¡° ê²€ì¦"""
        query = SearchQuery(text="ìë°”", mode=SearchMode.HYBRID, top_k=3)

        response = await ingested_service.search(query)

        for result in response.results:
            # í•„ìˆ˜ í•„ë“œ
            assert result.chunk_id
            assert result.text
            assert result.score >= 0
            assert result.book_id
            assert result.book_title
            assert result.page_start >= 1
```

---

## 7. Test Commands

### 7.1 ì‹¤í–‰ ëª…ë ¹

```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸
pytest

# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ë§Œ
pytest tests/unit/ -v

# í†µí•© í…ŒìŠ¤íŠ¸ë§Œ
pytest -m integration -v

# E2E í…ŒìŠ¤íŠ¸ë§Œ
pytest -m e2e -v

# íŠ¹ì • íŒŒì¼
pytest tests/unit/test_chunker.py -v

# íŠ¹ì • í…ŒìŠ¤íŠ¸
pytest tests/unit/test_chunker.py::TestTextChunker::test_basic_chunking -v

# ì»¤ë²„ë¦¬ì§€ ë¦¬í¬íŠ¸
pytest --cov=src/bookbrain --cov-report=html

# ëŠë¦° í…ŒìŠ¤íŠ¸ ì œì™¸
pytest -m "not slow"

# ì™¸ë¶€ ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸ ì œì™¸
pytest -m "not external"

# ë³‘ë ¬ ì‹¤í–‰
pytest -n auto
```

### 7.2 CI ì„¤ì •

```yaml
# .github/workflows/test.yml

name: Tests

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: |
          pip install poetry
          poetry install

      - name: Run unit tests
        run: |
          poetry run pytest tests/unit/ -v --cov=src/bookbrain

      - name: Run integration tests
        run: |
          poetry run pytest tests/integration/ -v -m "not external"

      - name: Upload coverage
        uses: codecov/codecov-action@v4
```

