# Ingestion Pipeline Specification

> **Role**: Technical Lead
> **Created**: 2025-12-04
> **Version**: 1.0

---

## 1. Pipeline Overview

PDF íŒŒì¼ì„ ë²¡í„° DBì— ì €ì¥í•˜ê¸°ê¹Œì§€ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ìƒì„¸íˆ ì •ì˜í•©ë‹ˆë‹¤.

### 1.1 ì „ì²´ íë¦„

```mermaid
flowchart TB
    subgraph Input["ğŸ“ ì…ë ¥"]
        PDF[PDF íŒŒì¼]
        Meta[ë©”íƒ€ë°ì´í„° JSON<br/>ì„ íƒì ]
    end

    subgraph Stage1["Stage 1: íŒŒì¼ ê²€ì¦"]
        Validate[íŒŒì¼ ê²€ì¦<br/>- ì¡´ì¬ ì—¬ë¶€<br/>- PDF í˜•ì‹<br/>- íŒŒì¼ í¬ê¸°<br/>- ì¤‘ë³µ ì²´í¬]
    end

    subgraph Stage2["Stage 2: ë¬¸ì„œ íŒŒì‹±"]
        Cache{ìºì‹œ<br/>ì¡´ì¬?}
        Upload[Storm Parse<br/>ì—…ë¡œë“œ]
        Poll[í´ë§<br/>ëŒ€ê¸°]
        Parse[íŒŒì‹± ê²°ê³¼<br/>ìˆ˜ì‹ ]
        CacheLoad[ìºì‹œ<br/>ë¡œë“œ]
        CacheSave[ìºì‹œ<br/>ì €ì¥]
    end

    subgraph Stage3["Stage 3: ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"]
        ExtractMeta[ë©”íƒ€ë°ì´í„°<br/>ì¶”ì¶œ]
        BookCreate[Book ê°ì²´<br/>ìƒì„±]
    end

    subgraph Stage4["Stage 4: ì²­í‚¹"]
        Chunk[í…ìŠ¤íŠ¸<br/>ì²­í‚¹]
        ChunkMeta[ì²­í¬ ë©”íƒ€ë°ì´í„°<br/>ë¶€ì°©]
    end

    subgraph Stage5["Stage 5: ì„ë² ë”©"]
        Batch[ë°°ì¹˜<br/>êµ¬ì„±]
        Embed[OpenAI<br/>ì„ë² ë”©]
        Attach[ë²¡í„°<br/>ë¶€ì°©]
    end

    subgraph Stage6["Stage 6: ì €ì¥"]
        ChromaStore[ChromaDB<br/>ì €ì¥]
        BM25Build[BM25<br/>ì¸ë±ìŠ¤]
        MetaStore[ë©”íƒ€ë°ì´í„°<br/>ì €ì¥]
    end

    subgraph Output["ğŸ“Š ì¶œë ¥"]
        Stats[ì²˜ë¦¬ í†µê³„]
        Log[ì²˜ë¦¬ ë¡œê·¸]
    end

    PDF --> Validate
    Meta --> ExtractMeta
    Validate --> Cache
    Cache -->|Yes| CacheLoad
    Cache -->|No| Upload
    Upload --> Poll
    Poll --> Parse
    Parse --> CacheSave
    CacheSave --> ExtractMeta
    CacheLoad --> ExtractMeta
    ExtractMeta --> BookCreate
    BookCreate --> Chunk
    Chunk --> ChunkMeta
    ChunkMeta --> Batch
    Batch --> Embed
    Embed --> Attach
    Attach --> ChromaStore
    Attach --> BM25Build
    BookCreate --> MetaStore
    ChromaStore --> Stats
    BM25Build --> Stats
    MetaStore --> Log
```

### 1.2 ë‹¨ê³„ë³„ ì…ì¶œë ¥ ì •ì˜

| Stage | ì…ë ¥ | ì¶œë ¥ | ì‹¤íŒ¨ ì‹œ |
|-------|------|------|---------|
| 1. íŒŒì¼ ê²€ì¦ | `Path` | `ValidatedFile` | `ValidationError` ë°œìƒ, ì¤‘ë‹¨ |
| 2. ë¬¸ì„œ íŒŒì‹± | `ValidatedFile` | `list[ParsedPage]` | `StormParseError` ë°œìƒ, ì¤‘ë‹¨ |
| 3. ë©”íƒ€ë°ì´í„° | `list[ParsedPage]` | `Book` | ê¸°ë³¸ê°’ ì‚¬ìš©, ê³„ì† |
| 4. ì²­í‚¹ | `Book`, `list[ParsedPage]` | `Iterator[Chunk]` | `ChunkingError` ë°œìƒ, ì¤‘ë‹¨ |
| 5. ì„ë² ë”© | `Iterator[Chunk]` | `Iterator[Chunk]` (with embedding) | ì¬ì‹œë„ 3íšŒ, ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨ |
| 6. ì €ì¥ | `list[Chunk]`, `Book` | `IngestionResult` | ë¡¤ë°± í›„ ì¤‘ë‹¨ |

---

## 2. Stage 1: íŒŒì¼ ê²€ì¦

### 2.1 ê²€ì¦ í•­ëª©

```python
@dataclass
class FileValidation:
    """íŒŒì¼ ê²€ì¦ ê²°ê³¼"""
    path: Path
    file_name: str
    file_size: int
    file_hash: str
    is_valid: bool
    errors: list[str]
```

| ê²€ì¦ | ì¡°ê±´ | ì—ëŸ¬ ë©”ì‹œì§€ |
|------|------|------------|
| ì¡´ì¬ ì—¬ë¶€ | `path.exists()` | `File not found: {path}` |
| íŒŒì¼ íƒ€ì… | `path.suffix.lower() == '.pdf'` | `Not a PDF file: {path}` |
| íŒŒì¼ í¬ê¸° | `0 < size <= 100MB` | `File too large: {size}MB (max 100MB)` |
| PDF í—¤ë” | `bytes[:4] == b'%PDF'` | `Invalid PDF format: {path}` |
| ì¤‘ë³µ ì²´í¬ | `hash not in existing_hashes` | `Duplicate file: {existing_title}` |

### 2.2 êµ¬í˜„ ì½”ë“œ

```python
# ingestion/validator.py

import hashlib
from pathlib import Path
from dataclasses import dataclass, field

from bookbrain.core.exceptions import ValidationError


@dataclass
class ValidatedFile:
    """ê²€ì¦ëœ íŒŒì¼ ì •ë³´"""
    path: Path
    file_name: str
    file_size: int
    file_hash: str


@dataclass
class ValidationResult:
    """ê²€ì¦ ê²°ê³¼"""
    is_valid: bool
    validated_file: ValidatedFile | None = None
    errors: list[str] = field(default_factory=list)


class FileValidator:
    """PDF íŒŒì¼ ê²€ì¦ê¸°"""

    MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB
    PDF_MAGIC_BYTES = b'%PDF'

    def __init__(self, existing_hashes: set[str] | None = None):
        """
        Args:
            existing_hashes: ì´ë¯¸ ë“±ë¡ëœ íŒŒì¼ í•´ì‹œ ì§‘í•© (ì¤‘ë³µ ì²´í¬ìš©)
        """
        self._existing_hashes = existing_hashes or set()

    def validate(self, path: Path) -> ValidationResult:
        """
        íŒŒì¼ ê²€ì¦ ìˆ˜í–‰

        Args:
            path: PDF íŒŒì¼ ê²½ë¡œ

        Returns:
            ValidationResult: ê²€ì¦ ê²°ê³¼
        """
        errors = []

        # 1. ì¡´ì¬ ì—¬ë¶€
        if not path.exists():
            errors.append(f"File not found: {path}")
            return ValidationResult(is_valid=False, errors=errors)

        # 2. íŒŒì¼ íƒ€ì…
        if path.suffix.lower() != '.pdf':
            errors.append(f"Not a PDF file: {path}")

        # 3. íŒŒì¼ í¬ê¸°
        file_size = path.stat().st_size
        if file_size == 0:
            errors.append(f"Empty file: {path}")
        elif file_size > self.MAX_FILE_SIZE:
            size_mb = file_size / (1024 * 1024)
            errors.append(f"File too large: {size_mb:.1f}MB (max 100MB)")

        # 4. PDF í—¤ë”
        if not errors:  # íŒŒì¼ ì½ê¸° ê°€ëŠ¥í•œ ê²½ìš°ë§Œ
            with open(path, 'rb') as f:
                header = f.read(4)
                if header != self.PDF_MAGIC_BYTES:
                    errors.append(f"Invalid PDF format: {path}")

        # 5. í•´ì‹œ ê³„ì‚° ë° ì¤‘ë³µ ì²´í¬
        if not errors:
            file_hash = self._compute_hash(path)

            if file_hash in self._existing_hashes:
                errors.append(f"Duplicate file detected (hash: {file_hash[:8]}...)")
            else:
                validated = ValidatedFile(
                    path=path,
                    file_name=path.name,
                    file_size=file_size,
                    file_hash=file_hash,
                )
                return ValidationResult(
                    is_valid=True,
                    validated_file=validated,
                )

        return ValidationResult(is_valid=False, errors=errors)

    def _compute_hash(self, path: Path) -> str:
        """SHA-256 í•´ì‹œ ê³„ì‚°"""
        sha256 = hashlib.sha256()
        with open(path, 'rb') as f:
            for chunk in iter(lambda: f.read(8192), b''):
                sha256.update(chunk)
        return sha256.hexdigest()
```

---

## 3. Stage 2: ë¬¸ì„œ íŒŒì‹±

### 3.1 ìºì‹± ì „ëµ

```mermaid
flowchart TB
    Start[íŒŒì‹± ì‹œì‘] --> CheckCache{ìºì‹œ í™•ì¸}

    CheckCache -->|ì¡´ì¬| ValidateCache{ìºì‹œ ìœ íš¨?}
    CheckCache -->|ì—†ìŒ| CallAPI[Storm API í˜¸ì¶œ]

    ValidateCache -->|ìœ íš¨| LoadCache[ìºì‹œ ë¡œë“œ]
    ValidateCache -->|ë§Œë£Œ/ì†ìƒ| CallAPI

    CallAPI --> SaveCache[ìºì‹œ ì €ì¥]
    SaveCache --> Return[ê²°ê³¼ ë°˜í™˜]
    LoadCache --> Return
```

### 3.2 ìºì‹œ êµ¬ì¡°

```
data/
â””â”€â”€ cache/
    â””â”€â”€ parse/
        â”œâ”€â”€ {file_hash}.json      # íŒŒì‹± ê²°ê³¼
        â””â”€â”€ {file_hash}.meta.json # ë©”íƒ€ë°ì´í„° (ë§Œë£Œ ì‹œê°„ ë“±)
```

**ìºì‹œ ë©”íƒ€ë°ì´í„°:**
```json
{
  "file_hash": "abc123...",
  "file_name": "book.pdf",
  "cached_at": "2025-12-04T10:00:00Z",
  "expires_at": "2026-12-04T10:00:00Z",
  "storm_job_id": "defa_...",
  "page_count": 100
}
```

### 3.3 êµ¬í˜„ ì½”ë“œ

```python
# ingestion/cache.py

import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional

from pydantic import BaseModel

from bookbrain.core.config import Settings
from bookbrain.models.api import StormPageResult


class CacheMetadata(BaseModel):
    """ìºì‹œ ë©”íƒ€ë°ì´í„°"""
    file_hash: str
    file_name: str
    cached_at: datetime
    expires_at: datetime
    storm_job_id: str
    page_count: int


class ParseCache:
    """íŒŒì‹± ê²°ê³¼ ìºì‹œ"""

    CACHE_DIR = "cache/parse"
    DEFAULT_TTL_DAYS = 365  # 1ë…„

    def __init__(self, settings: Settings):
        self._settings = settings
        self._cache_dir = Path(settings.data_dir) / self.CACHE_DIR
        self._cache_dir.mkdir(parents=True, exist_ok=True)

    def get(self, file_hash: str) -> Optional[list[StormPageResult]]:
        """
        ìºì‹œì—ì„œ íŒŒì‹± ê²°ê³¼ ì¡°íšŒ

        Args:
            file_hash: íŒŒì¼ SHA-256 í•´ì‹œ

        Returns:
            íŒŒì‹± ê²°ê³¼ ë˜ëŠ” None (ìºì‹œ ë¯¸ìŠ¤/ë§Œë£Œ)
        """
        meta_path = self._cache_dir / f"{file_hash}.meta.json"
        data_path = self._cache_dir / f"{file_hash}.json"

        if not meta_path.exists() or not data_path.exists():
            return None

        # ë©”íƒ€ë°ì´í„° ë¡œë“œ ë° ë§Œë£Œ í™•ì¸
        with open(meta_path, 'r', encoding='utf-8') as f:
            meta = CacheMetadata.model_validate_json(f.read())

        if datetime.now() > meta.expires_at:
            # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
            meta_path.unlink(missing_ok=True)
            data_path.unlink(missing_ok=True)
            return None

        # ë°ì´í„° ë¡œë“œ
        with open(data_path, 'r', encoding='utf-8') as f:
            pages_data = json.load(f)

        return [StormPageResult.model_validate(p) for p in pages_data]

    def set(
        self,
        file_hash: str,
        file_name: str,
        job_id: str,
        pages: list[StormPageResult],
        ttl_days: int | None = None,
    ) -> None:
        """
        íŒŒì‹± ê²°ê³¼ ìºì‹œ ì €ì¥

        Args:
            file_hash: íŒŒì¼ í•´ì‹œ
            file_name: ì›ë³¸ íŒŒì¼ëª…
            job_id: Storm Parse job ID
            pages: íŒŒì‹± ê²°ê³¼
            ttl_days: ìºì‹œ ìœ íš¨ ê¸°ê°„ (ì¼)
        """
        ttl = ttl_days or self.DEFAULT_TTL_DAYS
        now = datetime.now()

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        meta = CacheMetadata(
            file_hash=file_hash,
            file_name=file_name,
            cached_at=now,
            expires_at=now + timedelta(days=ttl),
            storm_job_id=job_id,
            page_count=len(pages),
        )

        meta_path = self._cache_dir / f"{file_hash}.meta.json"
        with open(meta_path, 'w', encoding='utf-8') as f:
            f.write(meta.model_dump_json(indent=2))

        # ë°ì´í„° ì €ì¥
        data_path = self._cache_dir / f"{file_hash}.json"
        with open(data_path, 'w', encoding='utf-8') as f:
            json.dump(
                [p.model_dump() for p in pages],
                f,
                ensure_ascii=False,
                indent=2,
            )

    def invalidate(self, file_hash: str) -> bool:
        """ìºì‹œ ë¬´íš¨í™”"""
        meta_path = self._cache_dir / f"{file_hash}.meta.json"
        data_path = self._cache_dir / f"{file_hash}.json"

        existed = meta_path.exists() or data_path.exists()
        meta_path.unlink(missing_ok=True)
        data_path.unlink(missing_ok=True)

        return existed

    def clear_expired(self) -> int:
        """ë§Œë£Œëœ ìºì‹œ ì •ë¦¬"""
        count = 0
        for meta_path in self._cache_dir.glob("*.meta.json"):
            try:
                with open(meta_path, 'r') as f:
                    meta = CacheMetadata.model_validate_json(f.read())

                if datetime.now() > meta.expires_at:
                    file_hash = meta.file_hash
                    self.invalidate(file_hash)
                    count += 1
            except Exception:
                continue

        return count
```

### 3.4 Storm Parse í˜¸ì¶œ ì‹œí€€ìŠ¤

```mermaid
sequenceDiagram
    participant P as Pipeline
    participant C as ParseCache
    participant S as StormClient
    participant API as Storm API

    P->>C: get(file_hash)

    alt ìºì‹œ íˆíŠ¸
        C-->>P: list[ParsedPage]
        Note over P: ìºì‹œ ì‚¬ìš©
    else ìºì‹œ ë¯¸ìŠ¤
        C-->>P: None

        P->>S: upload_pdf(path)
        S->>API: POST /parse/by-file
        API-->>S: {jobId, state: REQUESTED}
        S-->>P: job_id

        loop ìµœëŒ€ 5ë¶„
            P->>S: get_job_status(job_id)
            S->>API: GET /parse/job/{jobId}
            API-->>S: {state, pages?}

            alt state == COMPLETED
                S-->>P: list[ParsedPage]
                P->>C: set(file_hash, pages)
                Note over P: ì™„ë£Œ
            else state == FAILED
                S-->>P: StormParseError
                Note over P: ì—ëŸ¬ ì²˜ë¦¬
            else ì§„í–‰ ì¤‘
                Note over P: 2ì´ˆ ëŒ€ê¸°
            end
        end
    end
```

---

## 4. Stage 3: ë©”íƒ€ë°ì´í„° ì¶”ì¶œ

### 4.1 ì¶”ì¶œ ê·œì¹™

| í•„ë“œ | ì¶”ì¶œ ì†ŒìŠ¤ | í´ë°± |
|------|----------|------|
| `book_id` | íŒŒì¼ëª… ì •ê·œí™” | UUID ìƒì„± |
| `title` | 1) ë©”íƒ€ JSON, 2) íŒŒì¼ëª… | íŒŒì¼ëª… |
| `chapters` | ë§ˆí¬ë‹¤ìš´ `##` í—¤ë” | ë¹ˆ ë¦¬ìŠ¤íŠ¸ |
| `language` | 1) ë©”íƒ€ JSON, 2) í…ìŠ¤íŠ¸ ê°ì§€ | "ko" |

### 4.2 íŒŒì¼ëª… ì •ê·œí™” ê·œì¹™

```python
def normalize_book_id(file_name: str) -> str:
    """
    íŒŒì¼ëª… â†’ book_id ë³€í™˜

    ê·œì¹™:
    1. í™•ì¥ì ì œê±°
    2. í•œê¸€ â†’ ì˜ë¬¸ ìŒì—­ (optional, ë³µì¡ì„±ìœ¼ë¡œ ìƒëµ)
    3. íŠ¹ìˆ˜ë¬¸ì â†’ ì–¸ë”ìŠ¤ì½”ì–´
    4. ì—°ì† ì–¸ë”ìŠ¤ì½”ì–´ â†’ ë‹¨ì¼
    5. ì†Œë¬¸ì ë³€í™˜
    6. ì•ë’¤ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°

    ì˜ˆì‹œ:
    - "ëª¨ë˜ìë°”ì¸ì•¡ì…˜.pdf" â†’ "ëª¨ë˜ìë°”ì¸ì•¡ì…˜"
    - "Modern Java in Action (2nd).pdf" â†’ "modern_java_in_action_2nd"
    - "Spring Boot 3.0 ê°€ì´ë“œ.pdf" â†’ "spring_boot_3_0_ê°€ì´ë“œ"
    """
    import re

    # í™•ì¥ì ì œê±°
    name = file_name.rsplit('.', 1)[0]

    # íŠ¹ìˆ˜ë¬¸ì â†’ ì–¸ë”ìŠ¤ì½”ì–´ (í•œê¸€, ì˜ë¬¸, ìˆ«ì ì œì™¸)
    name = re.sub(r'[^\wê°€-í£]', '_', name)

    # ì—°ì† ì–¸ë”ìŠ¤ì½”ì–´ â†’ ë‹¨ì¼
    name = re.sub(r'_+', '_', name)

    # ì•ë’¤ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
    name = name.strip('_')

    # ì†Œë¬¸ì ë³€í™˜ (í•œê¸€ì€ ì˜í–¥ ì—†ìŒ)
    name = name.lower()

    return name
```

### 4.3 ì±•í„° ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜

```python
def extract_chapters(pages: list[ParsedPage]) -> list[Chapter]:
    """
    ë§ˆí¬ë‹¤ìš´ì—ì„œ ì±•í„° êµ¬ì¡° ì¶”ì¶œ

    ì¸ì‹ íŒ¨í„´:
    - ## ë˜ëŠ” # ë¡œ ì‹œì‘í•˜ëŠ” ì¤„
    - "ì œNì¥", "Chapter N", "Part N" íŒ¨í„´

    ì¤‘ë³µ ì œê±°:
    - ê°™ì€ ì œëª©ì´ ì—¬ëŸ¬ í˜ì´ì§€ì— ë‚˜ì˜¤ë©´ ì²« ë²ˆì§¸ë§Œ ì‚¬ìš©
    """
    import re

    chapters = []
    seen_titles = set()

    header_pattern = re.compile(r'^(#{1,2})\s+(.+)$', re.MULTILINE)
    chapter_pattern = re.compile(
        r'^(ì œ\s*\d+\s*ì¥|Chapter\s+\d+|Part\s+\d+|PART\s+\d+)',
        re.IGNORECASE
    )

    for page in pages:
        for match in header_pattern.finditer(page.content):
            level = len(match.group(1))  # # ê°œìˆ˜
            title = match.group(2).strip()

            # ì±•í„° íŒ¨í„´ ë§¤ì¹­ ë˜ëŠ” ë ˆë²¨ 1-2 í—¤ë”
            is_chapter = chapter_pattern.match(title) or level <= 2

            if is_chapter and title not in seen_titles:
                seen_titles.add(title)
                chapters.append(Chapter(
                    title=title,
                    level=level,
                    start_page=page.page_number,
                ))

    return chapters
```

### 4.4 êµ¬í˜„ ì½”ë“œ

```python
# ingestion/parser.py

import re
from typing import Optional

from bookbrain.core.config import Settings
from bookbrain.models.document import Book, Chapter, ParsedPage


class MetadataExtractor:
    """ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸°"""

    def __init__(self, settings: Settings):
        self._settings = settings

    def extract(
        self,
        file_name: str,
        file_hash: str,
        pages: list[ParsedPage],
        metadata_override: dict | None = None,
    ) -> Book:
        """
        Book ê°ì²´ ìƒì„±

        Args:
            file_name: ì›ë³¸ íŒŒì¼ëª…
            file_hash: íŒŒì¼ í•´ì‹œ
            pages: íŒŒì‹±ëœ í˜ì´ì§€ ëª©ë¡
            metadata_override: ìˆ˜ë™ ë©”íƒ€ë°ì´í„° (title, language ë“±)

        Returns:
            Book: ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„°ë¥¼ ë‹´ì€ Book ê°ì²´
        """
        override = metadata_override or {}

        # book_id
        book_id = self._normalize_book_id(file_name)

        # title (ìš°ì„ ìˆœìœ„: override > íŒŒì¼ëª…)
        title = override.get('title') or self._extract_title_from_filename(file_name)

        # chapters
        chapters = self._extract_chapters(pages)

        # ê° í˜ì´ì§€ì— ì±•í„° ì •ë³´ ì¶”ê°€
        pages = self._assign_chapters_to_pages(pages, chapters)

        # language
        language = override.get('language', 'ko')

        return Book(
            id=book_id,
            title=title,
            file_name=file_name,
            file_hash=file_hash,
            total_pages=len(pages),
            chapters=chapters,
            language=language,
        )

    def _normalize_book_id(self, file_name: str) -> str:
        """íŒŒì¼ëª… â†’ book_id"""
        name = file_name.rsplit('.', 1)[0]
        name = re.sub(r'[^\wê°€-í£]', '_', name)
        name = re.sub(r'_+', '_', name)
        name = name.strip('_').lower()
        return name or f"book_{hash(file_name) % 10000:04d}"

    def _extract_title_from_filename(self, file_name: str) -> str:
        """íŒŒì¼ëª…ì—ì„œ ì œëª© ì¶”ì¶œ"""
        name = file_name.rsplit('.', 1)[0]
        # ê´„í˜¸ ì•ˆ ë‚´ìš© ì œê±° (ì—ë””ì…˜ ì •ë³´ ë“±)
        name = re.sub(r'\s*\([^)]*\)\s*', ' ', name)
        # ì–¸ë”ìŠ¤ì½”ì–´/í•˜ì´í”ˆ â†’ ê³µë°±
        name = re.sub(r'[_-]', ' ', name)
        # ì—°ì† ê³µë°± ì •ë¦¬
        name = re.sub(r'\s+', ' ', name).strip()
        return name

    def _extract_chapters(self, pages: list[ParsedPage]) -> list[Chapter]:
        """ë§ˆí¬ë‹¤ìš´ì—ì„œ ì±•í„° ì¶”ì¶œ"""
        chapters = []
        seen_titles = set()

        header_pattern = re.compile(r'^(#{1,2})\s+(.+)$', re.MULTILINE)

        for page in pages:
            for match in header_pattern.finditer(page.content):
                level = len(match.group(1))
                title = match.group(2).strip()

                # ë„ˆë¬´ ì§§ì€ ì œëª© ë¬´ì‹œ
                if len(title) < 2:
                    continue

                # ì½”ë“œ ê´€ë ¨ í—¤ë” ë¬´ì‹œ
                if title.startswith('```'):
                    continue

                if title not in seen_titles:
                    seen_titles.add(title)
                    chapters.append(Chapter(
                        title=title,
                        level=level,
                        start_page=page.page_number,
                    ))

        return chapters

    def _assign_chapters_to_pages(
        self,
        pages: list[ParsedPage],
        chapters: list[Chapter],
    ) -> list[ParsedPage]:
        """ê° í˜ì´ì§€ì— í˜„ì¬ ì±•í„° ì •ë³´ í• ë‹¹"""
        if not chapters:
            return pages

        result = []
        current_chapter = None
        chapter_idx = 0

        for page in pages:
            # ì´ í˜ì´ì§€ì—ì„œ ìƒˆ ì±•í„°ê°€ ì‹œì‘ë˜ëŠ”ì§€ í™•ì¸
            while (
                chapter_idx < len(chapters) and
                chapters[chapter_idx].start_page <= page.page_number
            ):
                current_chapter = chapters[chapter_idx].title
                chapter_idx += 1

            result.append(ParsedPage(
                page_number=page.page_number,
                content=page.content,
                chapter_title=current_chapter,
            ))

        return result
```

---

## 5. Stage 4: í…ìŠ¤íŠ¸ ì²­í‚¹

### 5.1 ì²­í‚¹ ì„¤ì •

```python
CHUNKING_CONFIG = {
    # ê¸°ë³¸ ì„¤ì •
    "chunk_size": 800,           # í† í°
    "chunk_overlap": 200,        # í† í°

    # ë¶„í• ì ìš°ì„ ìˆœìœ„ (ë†’ì€ ê²ƒë¶€í„°)
    "separators": [
        "\n## ",                  # H2 í—¤ë”
        "\n### ",                 # H3 í—¤ë”
        "\n#### ",                # H4 í—¤ë”
        "\n\n",                   # ë¬¸ë‹¨
        "\n",                     # ì¤„ë°”ê¿ˆ
        "```\n",                  # ì½”ë“œ ë¸”ë¡ ë
        "\n```",                  # ì½”ë“œ ë¸”ë¡ ì‹œì‘
        ". ",                     # ë¬¸ì¥
        "ã€‚",                     # í•œê¸€ ë¬¸ì¥ (ì¼ë³¸ì–´ ìŠ¤íƒ€ì¼)
        ", ",                     # ì ˆ
        " ",                      # ë‹¨ì–´
        "",                       # ë¬¸ì (ìµœí›„)
    ],

    # í† í° ì¹´ìš´í„°
    "tokenizer": "cl100k_base",  # GPT-4/3.5 í† í¬ë‚˜ì´ì €

    # ì½”ë“œ ë¸”ë¡ ë³´í˜¸
    "preserve_code_blocks": True,

    # í…Œì´ë¸” ë³´í˜¸
    "preserve_tables": True,
}
```

### 5.2 ì½”ë“œ ë¸”ë¡ ë³´í˜¸ ë¡œì§

```python
def protect_code_blocks(text: str) -> tuple[str, dict[str, str]]:
    """
    ì½”ë“œ ë¸”ë¡ì„ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ì¹˜í™˜

    Returns:
        (ì¹˜í™˜ëœ í…ìŠ¤íŠ¸, í”Œë ˆì´ìŠ¤í™€ë”â†’ì›ë³¸ ë§¤í•‘)
    """
    import re
    import uuid

    code_pattern = re.compile(r'```[\s\S]*?```', re.MULTILINE)
    placeholders = {}

    def replace(match):
        placeholder = f"__CODE_BLOCK_{uuid.uuid4().hex[:8]}__"
        placeholders[placeholder] = match.group(0)
        return placeholder

    protected = code_pattern.sub(replace, text)
    return protected, placeholders


def restore_code_blocks(text: str, placeholders: dict[str, str]) -> str:
    """í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì›ë³¸ ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ ë³µì›"""
    for placeholder, original in placeholders.items():
        text = text.replace(placeholder, original)
    return text
```

### 5.3 ì²­í‚¹ íë¦„

```mermaid
flowchart TB
    Input[í˜ì´ì§€ ëª©ë¡] --> Concat[í…ìŠ¤íŠ¸ ì—°ê²°<br/>+ í˜ì´ì§€ ê²½ê³„ ê¸°ë¡]

    Concat --> Protect[ì½”ë“œ/í…Œì´ë¸”<br/>ë³´í˜¸ ì²˜ë¦¬]

    Protect --> Split[RecursiveCharacterTextSplitter<br/>ë¶„í• ]

    Split --> Restore[ì½”ë“œ/í…Œì´ë¸”<br/>ë³µì›]

    Restore --> Locate[í˜ì´ì§€ ë²”ìœ„<br/>ê³„ì‚°]

    Locate --> Meta[ë©”íƒ€ë°ì´í„°<br/>ë¶€ì°©]

    Meta --> Output[Chunk ëª©ë¡]
```

### 5.4 ì²­í¬ í’ˆì§ˆ ê²€ì¦

```python
def validate_chunk(chunk: Chunk) -> list[str]:
    """
    ì²­í¬ í’ˆì§ˆ ê²€ì¦

    Returns:
        ê²½ê³  ë©”ì‹œì§€ ëª©ë¡ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ë©´ OK)
    """
    warnings = []

    # ìµœì†Œ ê¸¸ì´
    if len(chunk.text) < 50:
        warnings.append(f"Chunk too short: {len(chunk.text)} chars")

    # ì½”ë“œ ë¸”ë¡ ë¶ˆì™„ì „
    if chunk.text.count('```') % 2 != 0:
        warnings.append("Unclosed code block")

    # ë¬¸ì¥ ì¤‘ê°„ ì ˆë‹¨ ê°ì§€
    if not chunk.text.rstrip().endswith(('.', 'ã€‚', '!', '?', '```', ':', ';')):
        # ë§ˆì§€ë§‰ ë¬¸ì¥ì´ ì™„ì „í•œì§€ íœ´ë¦¬ìŠ¤í‹± ì²´í¬
        last_line = chunk.text.rstrip().split('\n')[-1]
        if len(last_line) > 20 and not last_line.endswith(('ë‹¤', 'ìš”', 'í•¨')):
            warnings.append("Possible mid-sentence truncation")

    return warnings
```

---

## 6. Stage 5: ì„ë² ë”© ìƒì„±

### 6.1 ë°°ì¹˜ ì²˜ë¦¬ ì „ëµ

```mermaid
flowchart TB
    Input[ì²­í¬ ìŠ¤íŠ¸ë¦¼] --> Buffer[ë²„í¼<br/>100ê°œì”©]

    Buffer --> Full{ë²„í¼<br/>ê°€ë“?}

    Full -->|Yes| Batch[ë°°ì¹˜ ìš”ì²­]
    Full -->|No & ìŠ¤íŠ¸ë¦¼ ë| Batch
    Full -->|No & ì§„í–‰ ì¤‘| Buffer

    Batch --> API[OpenAI API<br/>í˜¸ì¶œ]

    API --> Success{ì„±ê³µ?}

    Success -->|Yes| Attach[ì„ë² ë”©<br/>ë¶€ì°©]
    Success -->|No| Retry{ì¬ì‹œë„<br/>ê°€ëŠ¥?}

    Retry -->|Yes| Wait[ì§€ìˆ˜ ë°±ì˜¤í”„<br/>ëŒ€ê¸°]
    Wait --> Batch
    Retry -->|No| Error[ì—ëŸ¬ ë°œìƒ]

    Attach --> Yield[ì²­í¬ ì¶œë ¥]
    Yield --> Buffer
```

### 6.2 Rate Limit ì²˜ë¦¬

```python
# OpenAI Rate Limits (text-embedding-3-small)
RATE_LIMITS = {
    "rpm": 3000,        # Requests per minute
    "tpm": 1_000_000,   # Tokens per minute
    "batch_size": 100,  # Max texts per request
}

# ì¬ì‹œë„ ì „ëµ
RETRY_CONFIG = {
    "max_attempts": 5,
    "initial_wait": 10,      # ì´ˆ
    "max_wait": 120,         # ì´ˆ
    "exponential_base": 2,
}
```

### 6.3 í† í° ì¶”ì •

```python
def estimate_tokens(texts: list[str]) -> int:
    """
    í…ìŠ¤íŠ¸ ëª©ë¡ì˜ ì´ í† í° ìˆ˜ ì¶”ì •

    ì •í™•í•œ ê³„ì‚°ì€ ë¹„ìš©ì´ í¬ë¯€ë¡œ íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš©:
    - í•œê¸€: ê¸€ìë‹¹ ~2 í† í°
    - ì˜ì–´: ë‹¨ì–´ë‹¹ ~1.3 í† í°
    - ì½”ë“œ: ë¬¸ìë‹¹ ~0.5 í† í°
    """
    import re

    total = 0
    for text in texts:
        # í•œê¸€ ë¬¸ì ìˆ˜
        korean = len(re.findall(r'[ê°€-í£]', text))
        # ì˜ì–´ ë‹¨ì–´ ìˆ˜
        english = len(re.findall(r'[a-zA-Z]+', text))
        # ë‚˜ë¨¸ì§€ ë¬¸ì
        other = len(text) - korean - english

        total += korean * 2 + english * 1.3 + other * 0.5

    return int(total)
```

---

## 7. Stage 6: ì €ì¥

### 7.1 íŠ¸ëœì­ì…˜ ì „ëµ

```python
@dataclass
class IngestionTransaction:
    """ìˆ˜ì§‘ íŠ¸ëœì­ì…˜"""

    book_id: str
    chunks_added: list[str] = field(default_factory=list)
    bm25_updated: bool = False
    metadata_saved: bool = False

    def rollback(
        self,
        chroma: ChromaStore,
        bm25: BM25Index,
        meta_store: MetadataStore,
    ) -> None:
        """ë¡¤ë°± ìˆ˜í–‰"""
        # 1. ChromaDB ì²­í¬ ì‚­ì œ
        if self.chunks_added:
            chroma.delete_chunks(self.chunks_added)

        # 2. BM25 ì¬êµ¬ì¶• (ì´ì „ ìƒíƒœë¡œ)
        if self.bm25_updated:
            bm25.rebuild_without(self.book_id)

        # 3. ë©”íƒ€ë°ì´í„° ì‚­ì œ
        if self.metadata_saved:
            meta_store.delete_book(self.book_id)
```

### 7.2 ì €ì¥ ìˆœì„œ

```mermaid
sequenceDiagram
    participant P as Pipeline
    participant T as Transaction
    participant C as ChromaDB
    participant B as BM25Index
    participant M as MetadataStore

    P->>T: begin(book_id)

    loop ì²­í¬ ë°°ì¹˜
        P->>C: add_chunks(batch)
        C-->>P: success
        P->>T: record_chunks(ids)
    end

    P->>B: add_chunks(all_chunks)
    B-->>P: success
    P->>T: record_bm25_update()

    P->>B: save()
    B-->>P: success

    P->>M: save_book(book)
    M-->>P: success
    P->>T: record_metadata()

    alt ëª¨ë“  ë‹¨ê³„ ì„±ê³µ
        P->>T: commit()
    else ì–´ëŠ ë‹¨ê³„ ì‹¤íŒ¨
        P->>T: rollback()
        T->>C: delete_chunks()
        T->>B: rebuild()
        T->>M: delete_book()
    end
```

### 7.3 ê²°ê³¼ ë¦¬í¬íŠ¸

```python
@dataclass
class IngestionResult:
    """ìˆ˜ì§‘ ê²°ê³¼"""

    success: bool
    book_id: str
    book_title: str

    # í†µê³„
    total_pages: int
    total_chunks: int
    total_tokens: int

    # íƒ€ì´ë°
    parse_time_sec: float
    chunk_time_sec: float
    embed_time_sec: float
    store_time_sec: float
    total_time_sec: float

    # ë¹„ìš© ì¶”ì •
    embedding_cost_usd: float
    parse_cost_usd: float  # Storm API

    # ê²½ê³ /ì—ëŸ¬
    warnings: list[str] = field(default_factory=list)
    error: str | None = None

    def to_dict(self) -> dict:
        """ë¦¬í¬íŠ¸ìš© ë”•ì…”ë„ˆë¦¬"""
        return {
            "status": "success" if self.success else "failed",
            "book": {
                "id": self.book_id,
                "title": self.book_title,
            },
            "stats": {
                "pages": self.total_pages,
                "chunks": self.total_chunks,
                "tokens": self.total_tokens,
            },
            "timing": {
                "parse": f"{self.parse_time_sec:.1f}s",
                "chunk": f"{self.chunk_time_sec:.1f}s",
                "embed": f"{self.embed_time_sec:.1f}s",
                "store": f"{self.store_time_sec:.1f}s",
                "total": f"{self.total_time_sec:.1f}s",
            },
            "cost": {
                "embedding": f"${self.embedding_cost_usd:.4f}",
                "parse": f"${self.parse_cost_usd:.4f}",
                "total": f"${self.embedding_cost_usd + self.parse_cost_usd:.4f}",
            },
            "warnings": self.warnings,
            "error": self.error,
        }
```

---

## 8. Pipeline Orchestrator

### 8.1 ì „ì²´ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤

```python
# ingestion/pipeline.py

import asyncio
import time
from pathlib import Path
from typing import Iterator, Callable

import structlog

from bookbrain.core.config import Settings
from bookbrain.core.exceptions import IngestionError
from bookbrain.ingestion.validator import FileValidator, ValidatedFile
from bookbrain.ingestion.storm_client import StormClient
from bookbrain.ingestion.cache import ParseCache
from bookbrain.ingestion.parser import MetadataExtractor
from bookbrain.ingestion.chunker import TextChunker
from bookbrain.ingestion.embedder import Embedder
from bookbrain.storage.chroma_store import ChromaStore
from bookbrain.storage.bm25_index import BM25Index
from bookbrain.models.document import Book
from bookbrain.models.chunk import Chunk


logger = structlog.get_logger()


class IngestionPipeline:
    """PDF ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸"""

    def __init__(
        self,
        settings: Settings,
        chroma_store: ChromaStore,
        bm25_index: BM25Index,
    ):
        self._settings = settings
        self._chroma = chroma_store
        self._bm25 = bm25_index

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._cache = ParseCache(settings)
        self._extractor = MetadataExtractor(settings)
        self._chunker = TextChunker(settings)
        self._embedder = Embedder(settings)

    async def ingest(
        self,
        pdf_path: Path,
        metadata_override: dict | None = None,
        progress_callback: Callable[[str, float], None] | None = None,
    ) -> IngestionResult:
        """
        PDF íŒŒì¼ ìˆ˜ì§‘ ì‹¤í–‰

        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            metadata_override: ìˆ˜ë™ ë©”íƒ€ë°ì´í„° (title, language ë“±)
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°± (stage, progress)

        Returns:
            IngestionResult: ìˆ˜ì§‘ ê²°ê³¼
        """
        start_time = time.perf_counter()
        warnings = []

        def report_progress(stage: str, progress: float):
            if progress_callback:
                progress_callback(stage, progress)

        try:
            # Stage 1: íŒŒì¼ ê²€ì¦
            report_progress("validating", 0.0)
            logger.info("ingestion_start", path=str(pdf_path))

            existing_hashes = self._get_existing_hashes()
            validator = FileValidator(existing_hashes)
            validation = validator.validate(pdf_path)

            if not validation.is_valid:
                raise IngestionError(
                    f"Validation failed: {', '.join(validation.errors)}"
                )

            validated = validation.validated_file
            report_progress("validating", 1.0)

            # Stage 2: ë¬¸ì„œ íŒŒì‹±
            report_progress("parsing", 0.0)
            parse_start = time.perf_counter()

            pages = await self._parse_with_cache(validated)

            parse_time = time.perf_counter() - parse_start
            report_progress("parsing", 1.0)
            logger.info("parsing_complete", pages=len(pages), time=parse_time)

            # Stage 3: ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            report_progress("extracting", 0.0)

            book = self._extractor.extract(
                file_name=validated.file_name,
                file_hash=validated.file_hash,
                pages=pages,
                metadata_override=metadata_override,
            )

            report_progress("extracting", 1.0)
            logger.info("metadata_extracted", book_id=book.id, title=book.title)

            # Stage 4: ì²­í‚¹
            report_progress("chunking", 0.0)
            chunk_start = time.perf_counter()

            chunks = list(self._chunker.chunk_pages(
                pages=pages,
                book_id=book.id,
                book_title=book.title,
                book_file=book.file_name,
            ))

            chunk_time = time.perf_counter() - chunk_start
            report_progress("chunking", 1.0)
            logger.info("chunking_complete", chunks=len(chunks), time=chunk_time)

            # Stage 5: ì„ë² ë”©
            report_progress("embedding", 0.0)
            embed_start = time.perf_counter()

            total_chunks = len(chunks)
            embedded_chunks = []

            i = 0
            async for chunk in self._embedder.embed_chunks(iter(chunks)):
                embedded_chunks.append(chunk)
                i += 1
                report_progress("embedding", i / total_chunks)

            embed_time = time.perf_counter() - embed_start
            logger.info("embedding_complete", time=embed_time)

            # Stage 6: ì €ì¥
            report_progress("storing", 0.0)
            store_start = time.perf_counter()

            # ChromaDB ì €ì¥
            self._chroma.add_chunks(embedded_chunks)
            report_progress("storing", 0.5)

            # BM25 ì¸ë±ìŠ¤ ì¶”ê°€
            self._bm25.add_chunks(embedded_chunks)
            self._bm25.save()
            report_progress("storing", 1.0)

            store_time = time.perf_counter() - store_start
            logger.info("storing_complete", time=store_time)

            # ê²°ê³¼ ìƒì„±
            total_time = time.perf_counter() - start_time
            total_tokens = sum(
                len(c.text.split()) * 1.5 for c in embedded_chunks  # ì¶”ì •
            )

            return IngestionResult(
                success=True,
                book_id=book.id,
                book_title=book.title,
                total_pages=len(pages),
                total_chunks=len(embedded_chunks),
                total_tokens=int(total_tokens),
                parse_time_sec=parse_time,
                chunk_time_sec=chunk_time,
                embed_time_sec=embed_time,
                store_time_sec=store_time,
                total_time_sec=total_time,
                embedding_cost_usd=total_tokens / 1000 * 0.00002,  # $0.00002/1K tokens
                parse_cost_usd=0.0,  # Storm ë¹„ìš© ëª¨ë¸ì— ë”°ë¼
                warnings=warnings,
            )

        except Exception as e:
            logger.error("ingestion_failed", error=str(e))
            return IngestionResult(
                success=False,
                book_id="",
                book_title="",
                total_pages=0,
                total_chunks=0,
                total_tokens=0,
                parse_time_sec=0,
                chunk_time_sec=0,
                embed_time_sec=0,
                store_time_sec=0,
                total_time_sec=time.perf_counter() - start_time,
                embedding_cost_usd=0,
                parse_cost_usd=0,
                error=str(e),
            )

    async def _parse_with_cache(
        self,
        validated: ValidatedFile,
    ) -> list:
        """ìºì‹œ í™•ì¸ í›„ íŒŒì‹±"""
        # ìºì‹œ í™•ì¸
        cached = self._cache.get(validated.file_hash)
        if cached:
            logger.info("cache_hit", file_hash=validated.file_hash[:8])
            return cached

        # Storm Parse í˜¸ì¶œ
        async with StormClient(self._settings) as client:
            pages = await client.parse_pdf(validated.path)

        # ìºì‹œ ì €ì¥
        self._cache.set(
            file_hash=validated.file_hash,
            file_name=validated.file_name,
            job_id="",  # ì‹¤ì œ job_id
            pages=pages,
        )

        return pages

    def _get_existing_hashes(self) -> set[str]:
        """ë“±ë¡ëœ íŒŒì¼ í•´ì‹œ ì¡°íšŒ"""
        # ë©”íƒ€ë°ì´í„° ì €ì¥ì†Œì—ì„œ ì¡°íšŒ
        # êµ¬í˜„ í•„ìš”
        return set()
```

### 8.2 CLI ì¸í„°í˜ì´ìŠ¤

```python
# main.py

import asyncio
from pathlib import Path
from typing import Optional

import typer
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn

from bookbrain.core.config import get_settings
from bookbrain.ingestion.pipeline import IngestionPipeline
from bookbrain.storage.chroma_store import ChromaStore
from bookbrain.storage.bm25_index import BM25Index


app = typer.Typer(help="BookBrain - Personal Library RAG System")
console = Console()


@app.command()
def ingest(
    pdf_path: Path = typer.Argument(..., help="PDF íŒŒì¼ ê²½ë¡œ"),
    title: Optional[str] = typer.Option(None, "--title", "-t", help="ì±… ì œëª© (ìˆ˜ë™ ì§€ì •)"),
    language: str = typer.Option("ko", "--lang", "-l", help="ë¬¸ì„œ ì–¸ì–´"),
):
    """PDF íŒŒì¼ì„ ë²¡í„° DBì— ìˆ˜ì§‘"""

    settings = get_settings()

    # ì €ì¥ì†Œ ì´ˆê¸°í™”
    chroma = ChromaStore(settings)
    chroma.initialize()

    bm25 = BM25Index(settings)
    bm25.load()  # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ

    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = IngestionPipeline(settings, chroma, bm25)

    metadata = {}
    if title:
        metadata["title"] = title
    metadata["language"] = language

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console,
    ) as progress:
        task = progress.add_task("ìˆ˜ì§‘ ì¤‘...", total=None)

        def on_progress(stage: str, pct: float):
            stage_names = {
                "validating": "íŒŒì¼ ê²€ì¦",
                "parsing": "ë¬¸ì„œ íŒŒì‹±",
                "extracting": "ë©”íƒ€ë°ì´í„° ì¶”ì¶œ",
                "chunking": "í…ìŠ¤íŠ¸ ì²­í‚¹",
                "embedding": "ì„ë² ë”© ìƒì„±",
                "storing": "ì €ì¥",
            }
            progress.update(task, description=f"{stage_names.get(stage, stage)} ({pct*100:.0f}%)")

        result = asyncio.run(
            pipeline.ingest(pdf_path, metadata, on_progress)
        )

    if result.success:
        console.print(f"\nâœ… [green]ìˆ˜ì§‘ ì™„ë£Œ[/green]")
        console.print(f"   ì±…: {result.book_title}")
        console.print(f"   í˜ì´ì§€: {result.total_pages}")
        console.print(f"   ì²­í¬: {result.total_chunks}")
        console.print(f"   ì†Œìš” ì‹œê°„: {result.total_time_sec:.1f}ì´ˆ")
    else:
        console.print(f"\nâŒ [red]ìˆ˜ì§‘ ì‹¤íŒ¨[/red]: {result.error}")
        raise typer.Exit(1)


@app.command()
def search(
    query: str = typer.Argument(..., help="ê²€ìƒ‰ ì¿¼ë¦¬"),
    top_k: int = typer.Option(10, "--top", "-k", help="ê²°ê³¼ ê°œìˆ˜"),
    mode: str = typer.Option("hybrid", "--mode", "-m", help="ê²€ìƒ‰ ëª¨ë“œ (hybrid/vector/keyword)"),
):
    """ì¥ì„œ ê²€ìƒ‰"""
    # ê²€ìƒ‰ ë¡œì§...
    pass


@app.command()
def stats():
    """ë¼ì´ë¸ŒëŸ¬ë¦¬ í†µê³„"""
    settings = get_settings()

    chroma = ChromaStore(settings)
    chroma.initialize()

    stats = chroma.get_stats()

    console.print(f"\nğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ í†µê³„")
    console.print(f"   ì´ ì±…: {stats['total_books']}ê¶Œ")
    console.print(f"   ì´ ì²­í¬: {stats['total_chunks']}ê°œ")
    console.print()

    for book in stats['books']:
        console.print(f"   â€¢ {book['title']}: {book['chunks']}ê°œ ì²­í¬")


if __name__ == "__main__":
    app()
```

---

## 9. ì—ëŸ¬ ë³µêµ¬ ì‹œë‚˜ë¦¬ì˜¤

### 9.1 Storm Parse ì‹¤íŒ¨

```mermaid
flowchart TB
    Start[íŒŒì‹± ì‹œì‘] --> Call[API í˜¸ì¶œ]
    Call --> Check{ì‘ë‹µ í™•ì¸}

    Check -->|429 Rate Limit| Wait1[60ì´ˆ ëŒ€ê¸°]
    Wait1 --> Retry1{ì¬ì‹œë„<br/>íšŸìˆ˜?}
    Retry1 -->|< 3| Call
    Retry1 -->|>= 3| Fail1[ì‹¤íŒ¨: Rate Limit]

    Check -->|5xx Server| Wait2[ì§€ìˆ˜ ë°±ì˜¤í”„]
    Wait2 --> Retry2{ì¬ì‹œë„<br/>íšŸìˆ˜?}
    Retry2 -->|< 3| Call
    Retry2 -->|>= 3| Fail2[ì‹¤íŒ¨: Server Error]

    Check -->|4xx Client| Fail3[ì¦‰ì‹œ ì‹¤íŒ¨]

    Check -->|FAILED ìƒíƒœ| Fail4[ì‹¤íŒ¨: Parse Error]

    Check -->|Timeout| Fail5[ì‹¤íŒ¨: Timeout]

    Check -->|COMPLETED| Success[ì„±ê³µ]
```

### 9.2 ì„ë² ë”© ì‹¤íŒ¨

```python
async def embed_with_recovery(
    texts: list[str],
    embedder: Embedder,
) -> list[list[float]]:
    """
    ì‹¤íŒ¨ ì‹œ ë°°ì¹˜ ë¶„í•  ì „ëµ

    1. ì „ì²´ ë°°ì¹˜ ì‹œë„ (100ê°œ)
    2. ì‹¤íŒ¨ ì‹œ ì ˆë°˜ìœ¼ë¡œ ë¶„í•  (50ê°œì”©)
    3. ê³„ì† ì‹¤íŒ¨ ì‹œ ê°œë³„ ì²˜ë¦¬ (1ê°œì”©)
    """
    batch_sizes = [100, 50, 25, 10, 1]

    for batch_size in batch_sizes:
        try:
            results = []
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i + batch_size]
                embeddings = await embedder.embed_texts(batch)
                results.extend(embeddings)
            return results
        except Exception as e:
            if batch_size == 1:
                raise
            logger.warning(
                "embedding_batch_failed",
                batch_size=batch_size,
                error=str(e),
            )
            continue

    raise EmbeddingError("All batch sizes failed")
```

### 9.3 ì €ì¥ ì‹¤íŒ¨ ë¡¤ë°±

```python
async def safe_store(
    chunks: list[Chunk],
    book: Book,
    chroma: ChromaStore,
    bm25: BM25Index,
) -> None:
    """íŠ¸ëœì­ì…˜ ì €ì¥"""

    stored_ids = []

    try:
        # ChromaDB ì €ì¥
        for batch in batched(chunks, 100):
            chroma.add_chunks(batch)
            stored_ids.extend([c.id for c in batch])

        # BM25 ì—…ë°ì´íŠ¸
        bm25.add_chunks(chunks)
        bm25.save()

    except Exception as e:
        # ë¡¤ë°±
        logger.error("storage_failed_rolling_back", error=str(e))

        if stored_ids:
            try:
                chroma.delete_chunks(stored_ids)
            except Exception:
                logger.error("rollback_failed")

        raise
```

